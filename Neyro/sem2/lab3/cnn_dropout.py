import numpy as np
import matplotlib.pyplot as plt
import pickle
from datetime import datetime
from google.colab import files
import warnings
warnings.filterwarnings('ignore')

# –£—Å—Ç–∞–Ω–æ–≤–∫–∞ —Ä—É—Å—Å–∫–æ–≥–æ —à—Ä–∏—Ñ—Ç–∞ –¥–ª—è matplotlib
plt.rcParams['font.family'] = 'DejaVu Sans'

class Dropout:
    def __init__(self, dropout_rate=0.5):
        self.dropout_rate = dropout_rate
        self.mask = None
        self.training = True
    
    def forward(self, x):
        if self.training:
            self.mask = (np.random.random(x.shape) > self.dropout_rate).astype(float)
            return x * self.mask / (1 - self.dropout_rate)
        else:
            return x
    
    def backward(self, doutput):
        if self.training:
            return doutput * self.mask / (1 - self.dropout_rate)
        else:
            return doutput

class SimpleDenseLayer:
    def __init__(self, input_size, output_size):
        self.weights = np.random.randn(input_size, output_size) * 0.1
        self.biases = np.zeros(output_size)
        self.input = None
    
    def forward(self, x):
        self.input = x
        return np.dot(x, self.weights) + self.biases
    
    def backward(self, doutput, learning_rate):
        batch_size = doutput.shape[0]
        dinput = np.dot(doutput, self.weights.T)
        dweights = np.dot(self.input.T, doutput)
        dbiases = np.sum(doutput, axis=0)
        self.weights -= learning_rate * dweights / batch_size
        self.biases -= learning_rate * dbiases / batch_size
        return dinput

class ReLU:
    def __init__(self):
        self.input = None
    
    def forward(self, x):
        self.input = x
        return np.maximum(0, x)
    
    def backward(self, doutput):
        dinput = doutput.copy()
        dinput[self.input <= 0] = 0
        return dinput

class Softmax:
    def __init__(self):
        self.output = None
    
    def forward(self, x):
        exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))
        self.output = exp_x / np.sum(exp_x, axis=1, keepdims=True)
        return self.output
    
    def backward(self, doutput):
        return doutput

class SimpleCNN:
    def __init__(self, learning_rate=0.01, dropout_rate=0.5):
        self.learning_rate = learning_rate
        self.dropout_rate = dropout_rate
        # –û—á–µ–Ω—å –ø—Ä–æ—Å—Ç–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ - —Ç–æ–ª—å–∫–æ –ø–æ–ª–Ω–æ—Å–≤—è–∑–Ω—ã–µ —Å–ª–æ–∏
        self.layers = [
            SimpleDenseLayer(28*28, 64),  # –ü—Ä—è–º–æ –∏–∑ 28x28 –≤ 64 –Ω–µ–π—Ä–æ–Ω–∞
            ReLU(),
            Dropout(dropout_rate),
            SimpleDenseLayer(64, 32),
            ReLU(),
            Dropout(dropout_rate),
            SimpleDenseLayer(32, 10),
            Softmax()
        ]
    
    def set_training(self, training=True):
        for layer in self.layers:
            if isinstance(layer, Dropout):
                layer.training = training
    
    def forward(self, x):
        # –í—ã—Ä–∞–≤–Ω–∏–≤–∞–µ–º –≤—Ö–æ–¥ —Å—Ä–∞–∑—É
        if len(x.shape) == 3:
            x = x.reshape(x.shape[0], -1)
        for layer in self.layers:
            x = layer.forward(x)
        return x
    
    def backward(self, doutput):
        for layer in reversed(self.layers):
            if isinstance(layer, SimpleDenseLayer):
                doutput = layer.backward(doutput, self.learning_rate)
            elif isinstance(layer, (ReLU, Softmax, Dropout)):
                doutput = layer.backward(doutput)
        return doutput
    
    def compute_loss(self, y_pred, y_true):
        m = y_true.shape[0]
        log_likelihood = -np.log(y_pred[np.arange(m), np.argmax(y_true, axis=1)])
        return np.sum(log_likelihood) / m
    
    def predict(self, x):
        self.set_training(False)
        probabilities = self.forward(x)
        self.set_training(True)
        return np.argmax(probabilities, axis=1)
    
    def accuracy(self, x, y):
        predictions = self.predict(x)
        true_labels = np.argmax(y, axis=1)
        return np.mean(predictions == true_labels)

def load_tiny_mnist():
    """–ó–∞–≥—Ä—É–∑–∫–∞ –æ—á–µ–Ω—å –º–∞–ª–µ–Ω—å–∫–æ–≥–æ –Ω–∞–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö"""
    try:
        import tensorflow as tf
        print("–ó–∞–≥—Ä—É–∑–∫–∞ tiny-MNIST...")
        (X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data()
        
        # –û–ß–ï–ù–¨ –º–∞–ª–µ–Ω—å–∫–∞—è —á–∞—Å—Ç—å –¥–∞–Ω–Ω—ã—Ö
        train_samples = 500   # –í—Å–µ–≥–æ 500 –ø—Ä–∏–º–µ—Ä–æ–≤!
        test_samples = 100
        
        X_train = X_train[:train_samples] / 255.0
        y_train = y_train[:train_samples]
        X_test = X_test[:test_samples] / 255.0
        y_test = y_test[:test_samples]
        
        # One-hot encoding
        y_train_onehot = np.eye(10)[y_train]
        y_test_onehot = np.eye(10)[y_test]
        
        print(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {X_train.shape[0]} —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã—Ö –∏ {X_test.shape[0]} —Ç–µ—Å—Ç–æ–≤—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π")
        return X_train, y_train_onehot, X_test, y_test_onehot
        
    except ImportError:
        print("–£—Å—Ç–∞–Ω–æ–≤–∫–∞ TensorFlow...")
        !pip install tensorflow -q
        import tensorflow as tf
        return load_tiny_mnist()

def split_validation_data(X, y, validation_ratio=0.2):
    num_validation = int(X.shape[0] * validation_ratio)
    indices = np.random.permutation(X.shape[0])
    train_indices = indices[num_validation:]
    val_indices = indices[:num_validation]
    X_train = X[train_indices]
    y_train = y[train_indices]
    X_val = X[val_indices]
    y_val = y[val_indices]
    return X_train, y_train, X_val, y_val

def ultra_fast_training():
    print("‚ö° –°–í–ï–†–•–ë–´–°–¢–†–û–ï –û–ë–£–ß–ï–ù–ò–ï CNN")
    print("="*45)
    
    X_train_full, y_train_full, X_test, y_test = load_tiny_mnist()
    X_train, y_train, X_val, y_val = split_validation_data(X_train_full, y_train_full, 0.2)
    
    print(f"–¢—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã–µ: {X_train.shape[0]}, –í–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—ã–µ: {X_val.shape[0]}, –¢–µ—Å—Ç–æ–≤—ã–µ: {X_test.shape[0]}")
    
    # –¢–µ—Å—Ç–∏—Ä—É–µ–º —Ä–∞–∑–Ω—ã–µ dropout rates
    dropout_rates = [0.0, 0.2, 0.4, 0.6]
    results = {}
    training_histories = {}
    
    for dropout_rate in dropout_rates:
        print(f"\n--- Dropout Rate: {dropout_rate} ---")
        cnn = SimpleCNN(learning_rate=0.02, dropout_rate=dropout_rate)  # –£–≤–µ–ª–∏—á–∏–ª–∏ learning rate
        
        epochs = 3  # –í—Å–µ–≥–æ 3 —ç–ø–æ—Ö–∏!
        batch_size = 16  # –û—á–µ–Ω—å –º–∞–ª–µ–Ω—å–∫–∏–π batch size
        
        train_acc_history = []
        val_acc_history = []
        train_loss_history = []
        
        for epoch in range(epochs):
            # –û—á–µ–Ω—å –±—ã—Å—Ç—Ä—ã–π training loop
            permutation = np.random.permutation(X_train.shape[0])
            X_shuffled = X_train[permutation]
            y_shuffled = y_train[permutation]
            
            epoch_loss = 0
            batch_count = 0
            
            # –¢–æ–ª—å–∫–æ 5 –±–∞—Ç—á–µ–π –¥–ª—è –°–£–ü–ï–† —Å–∫–æ—Ä–æ—Å—Ç–∏!
            for i in range(0, min(X_train.shape[0], 80), batch_size):
                X_batch = X_shuffled[i:i+batch_size]
                y_batch = y_shuffled[i:i+batch_size]
                y_pred = cnn.forward(X_batch)
                loss = cnn.compute_loss(y_pred, y_batch)
                epoch_loss += loss
                batch_count += 1
                doutput = y_pred - y_batch
                cnn.backward(doutput)
            
            avg_loss = epoch_loss / batch_count
            train_acc = cnn.accuracy(X_train[:100], y_train[:100])  # –¢–æ–ª—å–∫–æ 100 –ø—Ä–∏–º–µ—Ä–æ–≤ –¥–ª—è –æ—Ü–µ–Ω–∫–∏
            val_acc = cnn.accuracy(X_val[:50], y_val[:50])
            
            train_acc_history.append(train_acc)
            val_acc_history.append(val_acc)
            train_loss_history.append(avg_loss)
            
            print(f"–≠–ø–æ—Ö–∞ {epoch+1}: Loss = {avg_loss:.3f}, Train Acc = {train_acc:.3f}, Val Acc = {val_acc:.3f}")
        
        test_acc = cnn.accuracy(X_test[:50], y_test[:50])
        results[dropout_rate] = {
            'train_acc': train_acc_history[-1],
            'val_acc': val_acc_history[-1],
            'test_acc': test_acc,
            'final_loss': train_loss_history[-1]
        }
        training_histories[dropout_rate] = {
            'train_acc': train_acc_history,
            'val_acc': val_acc_history,
            'train_loss': train_loss_history
        }
        print(f"–§–∏–Ω–∞–ª—å–Ω–∞—è Test Accuracy: {test_acc:.3f}")
    
    return results, training_histories

def create_comprehensive_plots(results, training_histories):
    print("\nüìä –ö–û–ú–ü–õ–ï–ö–°–ù–ê–Ø –í–ò–ó–£–ê–õ–ò–ó–ê–¶–ò–Ø –†–ï–ó–£–õ–¨–¢–ê–¢–û–í")
    
    # –°–æ–∑–¥–∞–µ–º –±–æ–ª—å—à—É—é —Ñ–∏–≥—É—Ä—É —Å –º–Ω–æ–∂–µ—Å—Ç–≤–æ–º –≥—Ä–∞—Ñ–∏–∫–æ–≤
    fig = plt.figure(figsize=(20, 15))
    
    # –ì—Ä–∞—Ñ–∏–∫ 1: –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Ç–æ—á–Ω–æ—Å—Ç–∏ –ø–æ —ç–ø–æ—Ö–∞–º –¥–ª—è —Ä–∞–∑–Ω—ã—Ö dropout rates
    ax1 = plt.subplot(3, 4, 1)
    colors = ['red', 'blue', 'green', 'orange']
    for i, dropout_rate in enumerate(training_histories.keys()):
        history = training_histories[dropout_rate]
        ax1.plot(history['train_acc'], label=f'Dropout {dropout_rate}', 
                color=colors[i], marker='o', linewidth=2)
    
    ax1.set_title('Train Accuracy –ø–æ —ç–ø–æ—Ö–∞–º', fontsize=12, fontweight='bold')
    ax1.set_xlabel('–≠–ø–æ—Ö–∞')
    ax1.set_ylabel('Accuracy')
    ax1.legend()
    ax1.grid(True, alpha=0.3)
    ax1.set_ylim(0, 1)
    
    # –ì—Ä–∞—Ñ–∏–∫ 2: Validation accuracy –ø–æ —ç–ø–æ—Ö–∞–º
    ax2 = plt.subplot(3, 4, 2)
    for i, dropout_rate in enumerate(training_histories.keys()):
        history = training_histories[dropout_rate]
        ax2.plot(history['val_acc'], label=f'Dropout {dropout_rate}', 
                color=colors[i], marker='s', linewidth=2)
    
    ax2.set_title('Validation Accuracy –ø–æ —ç–ø–æ—Ö–∞–º', fontsize=12, fontweight='bold')
    ax2.set_xlabel('–≠–ø–æ—Ö–∞')
    ax2.set_ylabel('Accuracy')
    ax2.legend()
    ax2.grid(True, alpha=0.3)
    ax2.set_ylim(0, 1)
    
    # –ì—Ä–∞—Ñ–∏–∫ 3: –§–∏–Ω–∞–ª—å–Ω–∞—è test accuracy
    ax3 = plt.subplot(3, 4, 3)
    dropout_rates = list(results.keys())
    test_accs = [results[dr]['test_acc'] for dr in dropout_rates]
    
    bars = ax3.bar(dropout_rates, test_accs, alpha=0.7, color=colors[:len(dropout_rates)])
    ax3.set_title('–§–∏–Ω–∞–ª—å–Ω–∞—è Test Accuracy', fontsize=12, fontweight='bold')
    ax3.set_xlabel('Dropout Rate')
    ax3.set_ylabel('Accuracy')
    ax3.grid(True, alpha=0.3)
    ax3.set_ylim(0, 1)
    
    # –î–æ–±–∞–≤–ª—è–µ–º –∑–Ω–∞—á–µ–Ω–∏—è –Ω–∞ —Å—Ç–æ–ª–±—Ü—ã
    for bar, acc in zip(bars, test_accs):
        height = bar.get_height()
        ax3.text(bar.get_x() + bar.get_width()/2., height + 0.02,
                f'{acc:.3f}', ha='center', va='bottom', fontweight='bold')
    
    # –ì—Ä–∞—Ñ–∏–∫ 4: –°—Ä–∞–≤–Ω–µ–Ω–∏–µ train vs val accuracy –¥–ª—è –≤—Å–µ—Ö –º–æ–¥–µ–ª–µ–π
    ax4 = plt.subplot(3, 4, 4)
    final_train_accs = [results[dr]['train_acc'] for dr in dropout_rates]
    final_val_accs = [results[dr]['val_acc'] for dr in dropout_rates]
    
    x_pos = np.arange(len(dropout_rates))
    width = 0.35
    
    ax4.bar(x_pos - width/2, final_train_accs, width, label='Train Acc', alpha=0.7)
    ax4.bar(x_pos + width/2, final_val_accs, width, label='Val Acc', alpha=0.7)
    
    ax4.set_title('–§–∏–Ω–∞–ª—å–Ω–∞—è Train vs Validation Accuracy', fontsize=12, fontweight='bold')
    ax4.set_xlabel('Dropout Rate')
    ax4.set_ylabel('Accuracy')
    ax4.set_xticks(x_pos)
    ax4.set_xticklabels(dropout_rates)
    ax4.legend()
    ax4.grid(True, alpha=0.3)
    ax4.set_ylim(0, 1)
    
    # –ì—Ä–∞—Ñ–∏–∫ 5: –§—É–Ω–∫—Ü–∏–∏ –ø–æ—Ç–µ—Ä—å –ø–æ —ç–ø–æ—Ö–∞–º
    ax5 = plt.subplot(3, 4, 5)
    for i, dropout_rate in enumerate(training_histories.keys()):
        history = training_histories[dropout_rate]
        ax5.plot(history['train_loss'], label=f'Dropout {dropout_rate}', 
                color=colors[i], marker='d', linewidth=2)
    
    ax5.set_title('–§—É–Ω–∫—Ü–∏—è –ø–æ—Ç–µ—Ä—å –ø–æ —ç–ø–æ—Ö–∞–º', fontsize=12, fontweight='bold')
    ax5.set_xlabel('–≠–ø–æ—Ö–∞')
    ax5.set_ylabel('Loss')
    ax5.legend()
    ax5.grid(True, alpha=0.3)
    
    # –ì—Ä–∞—Ñ–∏–∫ 6: –†–∞–∑–Ω–∏—Ü–∞ –º–µ–∂–¥—É train –∏ val accuracy (overfitting)
    ax6 = plt.subplot(3, 4, 6)
    overfitting_gap = [final_train_accs[i] - final_val_accs[i] for i in range(len(dropout_rates))]
    
    bars = ax6.bar(dropout_rates, overfitting_gap, alpha=0.7, 
                  color=['red' if gap > 0.1 else 'green' for gap in overfitting_gap])
    ax6.set_title('–†–∞–∑–Ω–∏—Ü–∞ Train-Val (Overfitting)', fontsize=12, fontweight='bold')
    ax6.set_xlabel('Dropout Rate')
    ax6.set_ylabel('Train Acc - Val Acc')
    ax6.grid(True, alpha=0.3)
    
    # –ì—Ä–∞—Ñ–∏–∫ 7: –ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –ø–æ dropout rates (—Ä–∞–¥–∞—Ä–Ω–∞—è –¥–∏–∞–≥—Ä–∞–º–º–∞)
    ax7 = plt.subplot(3, 4, 7, polar=True)
    metrics = ['Train Acc', 'Val Acc', 'Test Acc', 'Generalization']
    num_vars = len(metrics)
    
    # –í—ã—á–∏—Å–ª—è–µ–º –∑–Ω–∞—á–µ–Ω–∏—è –¥–ª—è —Ä–∞–¥–∞—Ä–Ω–æ–π –¥–∏–∞–≥—Ä–∞–º–º—ã
    values = {}
    for dr in dropout_rates:
        values[dr] = [
            results[dr]['train_acc'],
            results[dr]['val_acc'], 
            results[dr]['test_acc'],
            min(results[dr]['val_acc'], results[dr]['test_acc'])  # –û–±–æ–±—â–∞—é—â–∞—è —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å
        ]
    
    angles = np.linspace(0, 2 * np.pi, num_vars, endpoint=False).tolist()
    angles += angles[:1]  # –ó–∞–º—ã–∫–∞–µ–º –∫—Ä—É–≥
    
    for i, dr in enumerate(dropout_rates):
        vals = values[dr]
        vals += vals[:1]  # –ó–∞–º—ã–∫–∞–µ–º –∫—Ä—É–≥
        ax7.plot(angles, vals, 'o-', linewidth=2, label=f'Dropout {dr}', color=colors[i])
        ax7.fill(angles, vals, alpha=0.1, color=colors[i])
    
    ax7.set_yticklabels([])
    ax7.set_xticks(angles[:-1])
    ax7.set_xticklabels(metrics)
    ax7.set_title('–°—Ä–∞–≤–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π\n(–†–∞–¥–∞—Ä–Ω–∞—è –¥–∏–∞–≥—Ä–∞–º–º–∞)', fontsize=12, fontweight='bold')
    ax7.legend(bbox_to_anchor=(1.1, 1.1))
    
    # –ì—Ä–∞—Ñ–∏–∫ 8: Heatmap —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏
    ax8 = plt.subplot(3, 4, 8)
    performance_matrix = np.array([
        [results[dr]['train_acc'] for dr in dropout_rates],
        [results[dr]['val_acc'] for dr in dropout_rates],
        [results[dr]['test_acc'] for dr in dropout_rates]
    ])
    
    im = ax8.imshow(performance_matrix, cmap='RdYlGn', aspect='auto', vmin=0, vmax=1)
    ax8.set_xticks(range(len(dropout_rates)))
    ax8.set_xticklabels(dropout_rates)
    ax8.set_yticks(range(3))
    ax8.set_yticklabels(['Train', 'Val', 'Test'])
    ax8.set_title('–ú–∞—Ç—Ä–∏—Ü–∞ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏', fontsize=12, fontweight='bold')
    
    # –î–æ–±–∞–≤–ª—è–µ–º –∑–Ω–∞—á–µ–Ω–∏—è –≤ heatmap
    for i in range(3):
        for j in range(len(dropout_rates)):
            text = ax8.text(j, i, f'{performance_matrix[i, j]:.2f}',
                           ha="center", va="center", color="black", fontweight='bold')
    
    # –ì—Ä–∞—Ñ–∏–∫ 9: –≠–≤–æ–ª—é—Ü–∏—è overfitting –ø–æ —ç–ø–æ—Ö–∞–º
    ax9 = plt.subplot(3, 4, 9)
    for i, dropout_rate in enumerate(training_histories.keys()):
        history = training_histories[dropout_rate]
        overfitting_epochs = [history['train_acc'][j] - history['val_acc'][j] 
                            for j in range(len(history['train_acc']))]
        ax9.plot(overfitting_epochs, label=f'Dropout {dropout_rate}', 
                color=colors[i], marker='^', linewidth=2)
    
    ax9.axhline(y=0, color='black', linestyle='--', alpha=0.5)
    ax9.set_title('–≠–≤–æ–ª—é—Ü–∏—è Overfitting', fontsize=12, fontweight='bold')
    ax9.set_xlabel('–≠–ø–æ—Ö–∞')
    ax9.set_ylabel('Train Acc - Val Acc')
    ax9.legend()
    ax9.grid(True, alpha=0.3)
    
    # –ì—Ä–∞—Ñ–∏–∫ 10: –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –≤—Å–µ—Ö –º–µ—Ç—Ä–∏–∫ –¥–ª—è –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏
    ax10 = plt.subplot(3, 4, 10)
    best_dropout = max(results.keys(), key=lambda x: results[x]['test_acc'])
    best_results = results[best_dropout]
    
    metrics_names = ['Train Acc', 'Val Acc', 'Test Acc', 'Loss']
    metrics_values = [
        best_results['train_acc'],
        best_results['val_acc'],
        best_results['test_acc'],
        1 - best_results['final_loss']  # –ò–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º loss –¥–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏
    ]
    
    bars = ax10.bar(metrics_names, metrics_values, alpha=0.7, color=['blue', 'green', 'red', 'purple'])
    ax10.set_title(f'–õ—É—á—à–∞—è –º–æ–¥–µ–ª—å\n(Dropout={best_dropout})', fontsize=12, fontweight='bold')
    ax10.set_ylabel('–ó–Ω–∞—á–µ–Ω–∏–µ')
    ax10.grid(True, alpha=0.3)
    ax10.set_ylim(0, 1)
    
    for bar, value in zip(bars, metrics_values):
        height = bar.get_height()
        ax10.text(bar.get_x() + bar.get_width()/2., height + 0.02,
                 f'{value:.3f}', ha='center', va='bottom', fontweight='bold')
    
    # –ì—Ä–∞—Ñ–∏–∫ 11: –°–∫–æ—Ä–æ—Å—Ç—å —Å—Ö–æ–¥–∏–º–æ—Å—Ç–∏
    ax11 = plt.subplot(3, 4, 11)
    for i, dropout_rate in enumerate(training_histories.keys()):
        history = training_histories[dropout_rate]
        ax11.plot(history['train_acc'], label=f'Dropout {dropout_rate}', 
                 color=colors[i], linewidth=2)
        # –û—Ç–º–µ—á–∞–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—É—é —Ç–æ—á–∫—É
        ax11.scatter(len(history['train_acc'])-1, history['train_acc'][-1], 
                    color=colors[i], s=100, zorder=5)
    
    ax11.set_title('–°–∫–æ—Ä–æ—Å—Ç—å —Å—Ö–æ–¥–∏–º–æ—Å—Ç–∏', fontsize=12, fontweight='bold')
    ax11.set_xlabel('–≠–ø–æ—Ö–∞')
    ax11.set_ylabel('Train Accuracy')
    ax11.legend()
    ax11.grid(True, alpha=0.3)
    ax11.set_ylim(0, 1)
    
    # –ì—Ä–∞—Ñ–∏–∫ 12: –°–≤–æ–¥–Ω–∞—è —Ç–∞–±–ª–∏—Ü–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    ax12 = plt.subplot(3, 4, 12)
    ax12.axis('off')
    
    # –°–æ–∑–¥–∞–µ–º —Ç–µ–∫—Å—Ç–æ–≤—É—é —Ç–∞–±–ª–∏—Ü—É
    table_data = []
    headers = ['Dropout', 'Train Acc', 'Val Acc', 'Test Acc', 'Loss']
    table_data.append(headers)
    
    for dr in dropout_rates:
        row = [
            f'{dr}',
            f'{results[dr]["train_acc"]:.3f}',
            f'{results[dr]["val_acc"]:.3f}',
            f'{results[dr]["test_acc"]:.3f}',
            f'{results[dr]["final_loss"]:.3f}'
        ]
        table_data.append(row)
    
    table = ax12.table(cellText=table_data, loc='center', cellLoc='center')
    table.auto_set_font_size(False)
    table.set_fontsize(10)
    table.scale(1, 2)
    ax12.set_title('–°–≤–æ–¥–Ω–∞—è —Ç–∞–±–ª–∏—Ü–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤', fontsize=12, fontweight='bold')
    
    plt.tight_layout()
    plt.savefig('comprehensive_results.png', dpi=120, bbox_inches='tight')
    plt.show()
    
    return best_dropout

def main():
    print("‚ö° –ó–ê–ü–£–°–ö –°–í–ï–†–•–ë–´–°–¢–†–û–ì–û –û–ë–£–ß–ï–ù–ò–Ø")
    print("–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –û–ß–ï–ù–¨ –º–∞–ª–µ–Ω—å–∫–∏–π –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö: 500 –ø—Ä–∏–º–µ—Ä–æ–≤")
    print("–û–∂–∏–¥–∞–µ–º–æ–µ –≤—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è: 30-60 —Å–µ–∫—É–Ω–¥ ‚ö°")
    print("="*55)
    
    # –°–≤–µ—Ä—Ö–±—ã—Å—Ç—Ä–æ–µ –æ–±—É—á–µ–Ω–∏–µ
    results, training_histories = ultra_fast_training()
    
    # –ö–æ–º–ø–ª–µ–∫—Å–Ω–∞—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è
    best_dropout = create_comprehensive_plots(results, training_histories)
    
    # –í—ã–≤–æ–¥ –∏—Ç–æ–≥–æ–≤
    print("\n" + "="*60)
    print("üéØ –ò–¢–û–ì–û–í–´–ï –†–ï–ó–£–õ–¨–¢–ê–¢–´:")
    print("="*60)
    for dropout_rate in sorted(results.keys()):
        res = results[dropout_rate]
        print(f"Dropout {dropout_rate}: Train={res['train_acc']:.3f}, "
              f"Val={res['val_acc']:.3f}, Test={res['test_acc']:.3f}, Loss={res['final_loss']:.3f}")
    
    print(f"\nüèÜ –õ–£–ß–®–ê–Ø –ú–û–î–ï–õ–¨: Dropout {best_dropout}")
    print(f"   Test Accuracy: {results[best_dropout]['test_acc']:.3f}")
    
    # –°–∫–∞—á–∏–≤–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    print("\nüì• –°–∫–∞—á–∏–≤–∞–Ω–∏–µ –∫–æ–º–ø–ª–µ–∫—Å–Ω–æ–≥–æ –≥—Ä–∞—Ñ–∏–∫–∞...")
    files.download('comprehensive_results.png')
    
    print("\n‚úÖ –û–ë–£–ß–ï–ù–ò–ï –ó–ê–í–ï–†–®–ï–ù–û! (–í—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è: ~30 —Å–µ–∫—É–Ω–¥)")

if __name__ == "__main__":
    main()
